import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as sc
from math import ceil, floor
import math
"""
Formule base di statistica 
"""

def mediana(array):		#Mediana, richiede in ingresso un array di numpy
	b = np.sort(array)
	n = len(array)
	if n % 2 == 0:
		median = (b[n//2 -1] + b[n//2]) / 2
	else:
		median = b[n//2]
	return median


def media(x): 			#MEDIA
	m = 0.0
	for i in x:
    		m += i # += incrementa la variabile somma di i
    		m /= len(x) # /= divide la variabile m per len(x)
	return m


def devst(x): 			#DEVIAZIONE #STANDARD
	s = 0.0
	N = len(x)
	m = media(x)
	for i in x:
  		s = s + (i-m)**2 
	v = math.sqrt(s/(N-1))
	return v


def varianza(x):		#VARIANZA CAMPIONARIA
	s = 0.0
	N = len(x)
	m = media(x)
	for i in x:
		s = s + (i-m)**2 
	v = math.sqrt(s/(N-1))
	return v**2


def erroreStandardMedia(x): 	#ERRORE STANDARD DELLA MEDIA
	N = len(x)
	e = devst(x)/math.sqrt(N)
	return e


def errore_varianza(array):
  """
  Dato un array, mi trova l'errore sulla varianza
  """
  var = np.var(array)
  N = len(array)
  var_err = np.sqrt(2*(var**2)/(N-1))
  return var_err


def percentile(x,p):
    """A function that determines the value above which lies the a certain fraction of the values.
    Args:
        x (numpy.ndarray): the array for which the percentile is to be calculated
        p (float): the fraction of the values (should be whithin [0,1])
    Returns:
        float: the value above which lies the fraction `p` of the values
    """
    if not (0 < p and p < 1):
        raise ValueError(f'The percentile value p = {p} is not within the range [0,1]')
    x_sorted = np.sort(x)
    idx = int(p*len(x))
    pv = x_sorted[idx]
    return p


def sturges (n):		#STURGES PER IL NUMERO DI BIN
	return int(np.ceil(1+3.322*np.log(n)))


def two_data_binning(data1, data2):
	"""Questa funzione, dati due set di dati diversi, mi trova
	il binning ottimale per plottare l'istogramma"""

	xMin = floor (min (min (data1), min (data2)))
	xMax = ceil (max (max (data1), max (data2)))
	N_bins = sturges (min (len (data1), len (data2)))
	bin_edges = np.linspace (xMin, xMax, N_bins)
	return bin_edges


def one_data_binning(data):
  	"""
	Preso un array di dati, mi produce i binedges e il bin_content;
  	Se voglio i primi N elementi, devo fare data_new = data[:N]
  	"""
  	N = len(data)
  	Nbins = sturges(N)
  	binnaggio, binedges = np.histogram(data, bins = Nbins, range = (min(data), max(data)))
  	return binnaggio, binedges


def plot_histogram(data, bins, label, color, edgecolor, xlabel, ylabel, overlay=None, density=True):
    """
    Crea un istogramma personalizzato con opzione di sovrapporre una curva teorica.
    Argomenti:
        data (array-like): Dati da plottare.
        bins (int): Numero di bin dell'istogramma.
        density (bool): Se True, normalizza l'istogramma.
        label (str): Etichetta dei dati.
        color (str): Colore dell'istogramma.
        overlay (callable): Funzione teorica da sovrapporre (opzionale), Se overlay
		non è nullo ci metto la funzione, cioè la pdf (senza l'argomento, solamente il nome della funzione)
    """
    plt.hist(data, bins=bins, density=density, alpha=0.6, color=color, label=label, edgecolor=edgecolor)
    if overlay is not None:
        x = np.linspace(min(data), max(data), 500)
        plt.plot(x, overlay(x), label="Theoretical", color='red')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel if density else "Count")
    plt.legend()
    plt.show()


def loglikelihood(pdf, theta, sample):
    """
    Calcola il logaritmo della likelihood dato un parametro theta (es. media) e un set di dati (sample).
    Argomenti:
      - pdf: funzione di densità di probabilità.
      - theta: parametro singolo (es. media).
      - sample: array di dati osservati.
    Ritorna:
      - log-likelihood (float).
    """
    logL = 0
    for x in sample:
      if (pdf(x, theta)) > 0:
        logL = logL + np.log(pdf(x, theta))
      else:
        raise ValueError(f"PDF value is zero or negative at x={x}, theta={theta}.")
    return logL


def loglikelihood(pdf, theta, sample):    #Logaritmo della likelihood
	risultato = 0
	for x in sample:
		if (pdf(x, theta) > 0):
			risultato += np.log(pdf(x, theta))
	return (risultato)


def maximumlikelihood(loglikelihood, pdf, sample, a, b, tolleranza=1e-6):
	phi = (1 + np.sqrt(5)) / 2
	x1 = a + (b - a) / phi**2
	x2 = a + (b - a) / phi

	while (b - a) > tolleranza:
		L1 = loglikelihood(pdf, x1, sample)
		L2 = loglikelihood(pdf, x2, sample)
		if L1 > L2:
			b = x2
			x2 = x1
			x1 = a + (b - a) / phi**2
		else:
			a = x1
			x1 = x2
			x2 = a + (b - a) / phi
	x_max = (a + b) / 2
	return x_max




import numpy as np
from iminuit import Minuit
from iminuit.cost import LeastSquares
from iminuit.cost import ExtendedBinnedNLL
from iminuit.cost import UnbinnedNLL
from iminuit.cost import BinnedNLL
from scipy.stats import chi2
import scipy.stats as sc

"""
Quando devo fare il chi2 con scipy, prima devo definire 
i dof(gradi di libertà):
  ndof = len(redshift) - len(params)
e il p-value:
  p_value = 1. - sc.chi2.cdf (chi2_min, df = ndof)
dove chi_min = m.fval, cioè il valore minimo del chiquadro dopo il processo di ottimizzazione 
QUINDI: devo prima ottimizzare con Minuit e migrad, poi posso accedere al valore di fval
"""




"""
Quando devo fittare dei dati non binnati con un modello, 
posso utilizzare i LeastSquares(x, y, s_y, modello). Il modello dei LeastSquares
prende la PDF, a differenza degli altri. 
  minimi = LeastSquares()
  m = Minuit(minimi, 'valore iniziali dei miei parametri')
  m.migrad()
  m.hesse()
  parametro_fit = m.values['parametro']
  parametro_err = m.errors['parametro']
  print(f"parametro : {parametro_fit:.2f} ± {parametro_err:.2f}")

Nel caso abbia più modelli e quindi più parametri, posso sistemarli in un array in questo modo
  modelli = [
      (D_lineare, {"H0": H0_start}),
      (D_sus, {"H": H0_start, "q": -1}),
      (D_cubic, {"Hc": H0_start, "qc": -1}),]       

Così ho messo in un dictionary tutti i parametri, e accedo ad essi usando   **params
"""



"""
Quando utilizzo misure binnate, posso usare o BinnedNLL o ExtendedBinnedNLL
La differenza è nel fatto che la Extended serve quando il numero di misure Ntot
non è noto o fixed, ma fa parte del fit
Uso BinnedNLL invece quando il numero di misure è noto e fisso 
Sia la Extended che la Binned usano come funzione modello la CDF, del tipo:
  def model_cdf(bin_edges, media, sigma):
    return sc.norm.cdf(bin_edges, media, sigma)
bin_edges va SEMPRE in prima posizione, in quanto non è mai (almeno credo) un parametro
Inoltre è meglio usare la funzione di numpy per i binedges
  bin_content, bin_edges = np.histogram(sample, Nbins=sturges(len(sample)), range = (min(sample, max(sample)))

Definisco poi una funzione costo, del tipo:
  funzione_costo = BinnedNLL(bin_content, bin_edges, model_cdf)
Uso poi Minuit, con la funzione costo e i parametri; nel mio esempio:
  m = Minuit(funzione_costo, media = np.mean(sample), sigma = np.std(sample))
Poi faccio la roba di iminuit
  m.migrad()
  m.minos()
  display(m)      OPPURE print(m.valid)
Per il p-value uso sempre il chi2
  if 1. - chi2.cdf (mL.fval, df = m.ndof) > 0.10:
    print ('the event sample is compatible with a Gaussian distribution')

QUINDI, a parte trattare Ntot come un parametro (quindi cambiano anche i gradi di libertà), non 
ci sono differenze pratiche tra Binned ed Extended
"""

"""
Quando non ho misure binnate posso anche in questo caso usare la NLL, solamente che uso la UnbinnedNLL
la Unbinned usa la PDF di una distribuzione, ad esempio:
  def model(x, media, sigma)
    return sc.norm.pdf(x, media, sigma)
  cost_function = Unbinned(sample, model)
  m = Minuit(cost_function, media = np.mean(sample), sigma = np.std(sample))
  m.migrad()
  assert m.valid
  display(m)

Nel caso voglia mettere un limite devo fare:
  m.limits['sigma'] = (0, nNone)
In questo modo ho lasciato libera la sigma e messo a zero la media
"""
